# ---------------- Name of the Workflow ----------------
name: ETL Pipeline Automation

# ---------------- When should it run? ----------------
on:
  schedule:
    - cron: "0 0 * * *"    # Run the workflow daily at 12:00 AM UTC
  workflow_dispatch:       # Allow manual run from GitHub UI

# ---------------- Authority to update Repository ----------------
permissions:
  contents: write

# ---------------- Set of steps to run ----------------
jobs:
  data-pipeline:
    runs-on: ubuntu-latest   # Use a Linux VM for the Job

    env: # Shared env variables available to all scripts
      DB_USER: ${{ secrets.DB_USER }}
      DB_PASS: ${{ secrets.DB_PASS }}
      DB_HOST: ${{ secrets.DB_HOST }}
      DB_NAME: ${{ secrets.DB_NAME }}

    steps:
      # ---------------- Step 1 : Checkout Code ----------------
      # This pulls your repository into the GitHub Runner VM
      - name: Checkout repository
        uses: actions/checkout@v3   
        with:
          token: ${{ secrets.GITHUB_TOKEN }}

      # ---------------- Step 2 : Set up Python ----------------
      # Installs Python 3.13.0 so GitHub can run your Scripts
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.13.0"

      # ---------------- Step 3 : Install Dependencies ----------------
      # Installs all Python libraries listed in requirements.txt
      - name: Install dependencies
        run: |
          pip install -r requirements.txt

      # ---------------- Step 4 : Run ETL Script ----------------
      # Runs your etl.py script to setup database
      - name: Run ETL Script
        run: |
          python scripts/etl.py

      # ---------------- Step 5 : Generate New Data and Append ----------------
      # Creates new data of customers and orders to ingest in the Neon Database
      - name: Run Generate Data Script
        run: |
          python scripts/generate_data.py

      # ---------------- Step 6 : Run Views Script ----------------
      # Creates or refreshes SQL views in Neon Database
      - name: Run Views Script
        run: |
          python scripts/create_views.py

      # ---------------- Step 7 : Export Views to CSV ----------------
      # Exports SQL views as CSVs inside the `views` folder
      - name: Run Export Views Script
        run: |
          python scripts/export_views.py

      # ---------------- Step 8 : Save CSVs as Artifacts ----------------
      # Stores CSVs in the workflow run, downloadable from GitHub
      - name: Upload Exported CSV as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: exported-views
          path: views/*.csv

      # ---------------- Step 9 : Commit CSVs to Repository ----------------
      # Updates repository with the latest CSVs so they are visible directly
      - name: Commit and Push CSVs
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add -A views/
          git commit -m "chore: update csv export from views on $(date -u +'%Y-%m-%d')" -m "[skip ci]" || echo "No changes to commit"
          git push

      # ---------------- Step 10 : Save Logs as Artifacts ----------------
      # Stores Logs in the workflow run, downloadable from GitHub
      - name: Upload Logs as Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: etl-logs
          path: logs/*.log